{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "Reinforcement learning is een interessante methode om machine learning problemen mee op te lossen. Net als bij supervised learning wordt feedback en parameter tuning gebruikt om het algoritme te verbeteren. Maar het grote verschil is dat er bij reinforcement learning vrijwel geen datasets en labels aan te pas komen.\n",
    "\n",
    "Reinforcement learning lijkt in veel opzichten op een toepassing in het brede veld van kunstmatige intelligentie (ook bekend als Artifical Intelligence (AI)). Met reinforcement learning proberen we meestal het algoritme een bepaald gedrag aan te leren (als situatie X zich voordoet, doe dan Y). Het kan echter niet volledig autonoom problemen oplossen, want er is nog altijd feedback van de ontwikkelaar nodig om het algoritme in de juiste richting te sturen. Aan het einde van de dag is reinforcement learning slechts één van de vele manieren waarmee problemen kunnen worden opgelost.\n",
    "\n",
    "En over problemen gesproken..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackjack\n",
    "\n",
    "Stel je voor, je schuift aan tafel en doet gezellig mee aan een potje blackjack. Je kent de regels niet, en de Dealer zegt: \"U heeft in totaal 20 punten. De Dealer heeft 10 punten. Wilt u uw punten vastzetten, of wilt u nog een kaart?\" Je vraag om nog een kaart, en de Dealer zegt dat je hebt verloren en pakt al jouw inleg.\n",
    "\n",
    "Dit is een pijnlijke les. Als deze exacte situatie zich nog een keer voordoet, dan zal je de volgende keer zeer waarschijnlijk voor de andere optie kiezen. Zodoende houd je bij wat de beste opties zijn in specifieke situaties. Deze methode noemen we reinforcement learning, waar we leren om de juiste acties te kiezen gebaseerd op de resultaten van eerder uitgevoerde acties in dezelfde situatie.\n",
    "\n",
    "Voordat we de opdracht uitleggen zullen we eerst wat veelgebruikte termen binnen reinforcement learning definiëren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminologie\n",
    "\n",
    "De eerste term is `agent`. De algoritmes die je in de eerste opdracht hebt toegepast reageerden passief op de data die ze kregen aangereikt: de data wordt geconsumeerd, berekeningen worden gedaan, en een classificatielabel wordt teruggegeven. Bij reinforcement learning hebben de algoritmes een actieve invloed op de *omgeving* waarin ze worden toegepast (dit noemen we de `environment`).\n",
    "\n",
    "De `agent` is het gedeelte van het algoritme dat keuzes maakt welke *actie* (dit noemen we `action`) wordt uitgevoerd, en deze `action` beïnvloed vervolgens de `environment`. Het algoritme maakt deze keuze door de huidige *status van de omgeving* (dit noemen we `state`) te analyseren. Oftewel de `state` bevat alle nodige informatie voor het algoritme om tot een keuze voor de volgende `action` te kunnen komen.\n",
    "\n",
    "Tot slot wordt er voor elke uitgevoerde actie een score (dit noemen we `reward`) toegekend. Als het resultaat van de actie positief heeft uitgepakt dan wordt een positieve `reward` toegekend, en anders een negatieve. Het is aan de ontwikkelaar om een keuze te maken hoe groot/klein, positief/negatief de `reward` hoort te zijn. Het doel is om de `reward` zo in te richten dat het algoritme leert wat de beste `action` is om uit te voeren in een specifieke `state`.\n",
    "\n",
    "#### Voorbeeld\n",
    "Laten we de termen toepassen in het bovengenoemde voorbeeld van blackjack. Het spel blackjack en alle daarbij behorende objecten (kaarten) en regels vormden in dit geval de `environment`. De `agent` was jijzelf, omdat jij de keuzes maakte die de `environment` beïnvloedde. De kaarten in jouw hand en de openliggende kaart van de dealer vormden de `state`. De `actions` die jij kon kiezen waren *\"geef me een kaart\"* of *\"zet mijn punten vast\"*. Uiteindelijk koos je voor een actie waarmee je jouw inzet verloor, dus de `reward` van deze `action` in deze `state` was gelijk aan het verlies van jouw inzet, oftewel een negatieve `reward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De opdracht\n",
    "\n",
    "Het doel van deze opdracht is om een algoritme te schrijven dat kan leren hoe het blackjack zo optimaal mogelijk kan spelen. Om dit te bereiken volg je de volgende stappen:\n",
    "\n",
    "1. Verken de code, objecten en libraries waarmee je gaat werken\n",
    "1. Bepaal de `state` voor dit spel\n",
    "1. Bepaal de `rewards` die je wilt toekennen aan de verscheidene uitkomsten van een uitgevoerde `action`\n",
    "1. Bepaal passende leerparameters voor het trainen van het algoritme\n",
    "1. Test de kwaliteit van het algoritme\n",
    "\n",
    "### Aangepaste regels\n",
    "Om de opdracht niet te complex te maken, hebben we het spel blackjack vereenvoudigd met de volgende regels:\n",
    "\n",
    "- Na elke ronde gaan de gespeelde kaarten weer terug in de stapel kaarten. Elke ronde wordt dus altijd met 52 kaarten gespeeld zodat de initiële `state` van elke ronde vergelijkbaar blijft.\n",
    "- Het plaatsen van een inzet wordt buiten beschouwing gelaten.\n",
    "- De enige acties die mogelijk zijn, zijn `HIT` en `STAND`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 1: Verken de code, objecten en libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from blackjack import Game, PlayerVictoryState, PlayerAction, DealerPlayStrategy, get_action_name\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "ACTIONS = [PlayerAction.HIT, PlayerAction.STAND]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als je de code hierboven uitvoert, worden de nodige objecten en libraries ingeladen waarmee je de opdracht kunt uitvoeren. De acties zijn ook alvast gedefinieerd.\n",
    "\n",
    "*Numpy* wordt gebruikt vanwege de krachtige `np.array([])` arrays die een hoop extra functionaliteit ondersteunt ten opzichte van de standaard arrays. *Pandas* wordt gebruikt om data in een DataFrame (een tabel / matrix) te kunnen tonen. *Matplotlib* en *Seaborn* bieden de mogelijkheid om grafieken te visualiseren zodat je een beter inzicht in de data kunt verkrijgen.   \n",
    "\n",
    "Uit de `blackjack` module worden een aantal classes, enums en een functie geïmporteerd. Je kunt de `help()` methode gebruiken om te zien wat er in deze objecten aanwezig is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Game in module blackjack:\n",
      "\n",
      "class Game(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dealer_strategy)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  act(self, action)\n",
      " |      The actor performs the chosen action.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      action : PlayerAction\n",
      " |          The action to perform.\n",
      " |              \n",
      " |      Returns\n",
      " |      -------\n",
      " |      [PlayerState, RoundState]\n",
      " |          The game's state from the player's perspective and the round state.\n",
      " |  \n",
      " |  deal_card(self)\n",
      " |  \n",
      " |  deal_initial_hands(self)\n",
      " |  \n",
      " |  get_round_end_state(self)\n",
      " |  \n",
      " |  hit(self, player=None)\n",
      " |  \n",
      " |  next_round(self)\n",
      " |      Start a new round. Set the participating players and deal the initial cards.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      PlayerState\n",
      " |          The game's state from the player's perspective.\n",
      " |  \n",
      " |  stand(self)\n",
      " |  \n",
      " |  update_player_state(self, drawn_card=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we een nieuw blackjack spel aanmaken. Om een nieuw spel aan te maken moet je de Dealer strategie opgeven. De mogelijke opties zijn:\n",
    "\n",
    "- `DealerPlayStrategy.REACH17`: Dit is een populaire strategie waarbij de Dealer probeert de speler te verslaan of gelijkspel te spelen door maximaal 17 punten te halen.\n",
    "- `DealerPlayStrategy.GREEDY`: De Dealer probeert koste wat kost om de speler te verslaan of gelijkspel te spelen.\n",
    "\n",
    "We zullen beginnen met de eerste strategie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(DealerPlayStrategy.REACH17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 2: Bepaal de state voor dit spel \n",
    "\n",
    "Zoals je waarschijnlijk al hebt gezien, geeft de game logic bij het starten van een nieuwe ronde een `PlayerState` object terug. Dit object representeert de `state` van het spel vanuit het perspectief van de `actor`.\n",
    "\n",
    "***OPDRACHT:*** *Bekijk deze state nader en creëer een nieuw state object die alle nodige informatie bevat die jij nodig acht om het algoritme een juiste `action` te kunnen laten kiezen. Tip: gebruik de `__dict__()` methode om de properties van een object te zien.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_state = game.next_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self, player_state):\n",
    "        \"\"\"\n",
    "        Initialize the State class and select the useful data from the player_state parameter.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        player_state : PlayerState\n",
    "            The game's state from the actor's perspective\n",
    "        \"\"\"\n",
    "        # TODO : Define your state\n",
    "        self.my_useful_property1 = None\n",
    "        self.my_useful_property2 = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Create a textual representation of the state.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            A textual representation of the state.\n",
    "        \"\"\"\n",
    "        # TODO : Return a textual representation of your chosen state\n",
    "        return \"{}_{}\".format(self.my_useful_property1, self.my_useful_property2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De onderstaande code levert de textuele representatie van jouw gedefinieerde `state` op. Als dit alle nodige informatie bevat, ga dan door naar stap 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(State(player_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 3: Bepaal de rewards die je wilt toekennen aan de verscheidene uitkomsten van een uitgevoerde actie\n",
    "\n",
    "Er zijn twee redenen waarom we juist de uitkomsten van de acties willen beoordelen in plaats van de actie zelf:\n",
    "\n",
    "1. De waarde van de acties willen we juist meten. Als we de waarde van de actie bij elke `state` al weten, dan is er geen reden om reinforcement learning toe te passen.\n",
    "1. De waarde van de acties kunnen wijzigen, zelfs als de `state` hetzelfde blijft. Bij blackjack staat namelijk nooit vast welke kaart je zal krijgen.\n",
    "\n",
    "De onderstaande code geeft de uitkomsten van een actie terug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_state_, round_state = game.act(PlayerAction.STAND)\n",
    "your_defined_player_state = State(player_state_)\n",
    "print(\"Actor state:\")\n",
    "print()\n",
    "print(your_defined_player_state)\n",
    "print()\n",
    "print(\"Round state:\")\n",
    "print()\n",
    "print(round_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De `round_state` bevat extra informatie die je kunt gebruiken om de `reward` van de uitkomst van een actie te kunnen berekenen.\n",
    "\n",
    "***OPDRACHT:*** *Implementeer de onderstaande functie voor het berekenen van een `reward` behorend bij de uitkomst van een gekozen actie. Je mag ook hulpfuncties of variabelen buiten de functie aanmaken als je dat zou willen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : variabelen / hulpfuncties\n",
    "    \n",
    "def calculate_reward(): # TODO : add parameters\n",
    "    \"\"\"\n",
    "    Calculate a positive, negative or neutral reward that fits the outcome of the chosen action.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ???\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The reward that fits the outcome of the chosen action.\n",
    "    \"\"\"\n",
    "    return 0.0 # TODO : add your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stap 4. Bepaal passende leerparameters voor het trainen van het algoritme\n",
    "\n",
    "Om het algoritme blackjack optimaal te kunnen laten spelen, moet het eerst kennis opbouwen door een aantal potjes te spelen. De eerste paar potjes heeft het algoritme geen idee wat de waarde is van de mogelijke acties bij de desbetreffende `states`. Naarmate er genoeg rondes zijn gespeeld kan het algoritme een strategie gaan vormen waarmee het blackjack zo optimaal mogelijk kan gaan spelen.\n",
    "\n",
    "#### Exploration en exploitation\n",
    "Bij reinforcement learning zijn er twee concepten die je kunt inzetten om het algoritme een strategie aan te laten leren:\n",
    "\n",
    "- `Exploration`. Het algoritme kiest een willekeurige actie om te meten in hoeverre dit positief of negatief uitpakt. Oftewel, het algoritme verkent mogelijke alternatieve strategieën.\n",
    "- `Exploitation`. Het algoritme benut de kennis die het heeft opgebouwd en kiest de actie met de hoogste `reward`. Oftewel, het algoritme hanteert de huidige strategie.\n",
    "\n",
    "Het is op het begin noodzakelijk om via `exploration` kennis op te bouwen. Naarmate het algoritme meer kennis opbouwt, wil je `exploitation` steeds meer laten toepassen. Deze populaire strategie staat bekend als het `epsilon-greedy` algoritme.\n",
    "\n",
    "Bij `epsilon-greedy` geeft de waarde `epsilon` de mate waarin `exploration` wordt toegepast aan. Een waarde van `epsilon = 0.9` geeft aan dat het algoritme 90% van de tijd `exploration` toepast, en de andere 10% van de tijd `exploitation`. Naar verloop van tijd kun je `epsilon` verlagen (bijvoorbeeld met een constante zogeheten `decay rate`) zodat de mate van `exploration` afneemt en `exploitation` toeneemt om tot een optimale strategie te komen. Het is niet makkelijk om deze balans goed te krijgen en het vereist enige *trial and error*.\n",
    "\n",
    "#### De leerparameters\n",
    "\n",
    "Hieronder volgen de leerparameters die nog door jou moeten worden geconfigureerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 100 # the number of rounds to play to train the algorithm \n",
    "\n",
    "epsilon = 1.0            # the exploration rate (0.0 - 1.0)\n",
    "epsilon_decay = 1.0      # the exploration decay rate (0.0 - 1.0)\n",
    "min_epsilon = 0.0        # the minimum exploration rate (0.0 - 1.0)\n",
    "\n",
    "learn_rate = 1.0         # determines to what proportion to weigh in the prior (1 - learn_rate) and new knowledge (0.0 - 1.0)\n",
    "learn_rate_decay = 1.0   # 0.0 - 1.0\n",
    "min_learn_rate = 0.0     # 0.0 - 1.0\n",
    "\n",
    "discount_factor = 1 # determines to what proportion to weigh in the future reward (0.0 - 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals je ziet zijn er nog twee nieuwe parameters bijgekomen:\n",
    "\n",
    "- `learn rate`. Ook wel bekend als alpha ($\\alpha$). De `learn rate` geeft aan in hoeverre de huidige en nieuwe kennis over de waarde van een status-actie paar wordt meegenomen.\n",
    "- `discount factor`. Ook wel bekend als gamma ($\\gamma$). De `discount factor` geeft aan in hoeverre een mogelijke toekomstige `reward` wordt meegenomen.\n",
    "\n",
    "Om beter te begrijpen hoe deze parameters worden toegepast, volgt hieronder de formule waarmee het algoritme zijn kennis opbouwt.\n",
    "\n",
    "$$Q^{new}(s_t, a_t)\\leftarrow (1 - \\alpha) \\cdot Q(s_t, a_t) + \\alpha \\cdot (r + \\gamma \\cdot max_aQ(s_{t+1}, a_t))$$\n",
    "\n",
    "Het ziet er complex uit, maar het is gemakkelijk te begrijpen. Wat er gebeurt is dat een nieuwe status-actie waarde wordt berekend op basis van de som van twee delen.\n",
    "\n",
    "Het eerste deel $(1 - \\alpha) \\cdot Q(s_t, a_t)$ vertegenwoordigt de oude status-actie waarde ($Q(s_t, a_t)$), oftewel de huidige kennis. $(1 - \\alpha)$ geeft aan hoe zwaar deze huidige kennis wordt meegewogen.\n",
    "\n",
    "Het tweede deel $\\alpha \\cdot (r + \\gamma \\cdot max_aQ(s_{t+1}, a_t))$ houdt de nieuw gevonden kennis in. Dit is gelijk aan de som van de directe beloning ($r$) die we hebben gekregen door de actie die is ondernomen + de maximale beloning die voor de volgende status kan worden verkregen ($max_aQ(s_{t+1}, a_t)$) en hoe zwaar dit wordt meegewogen door $\\gamma$ (de `discount factor`). $\\alpha$ geeft tot slot aan hoe zwaar deze nieuwe kennis wordt meegewogen.\n",
    "\n",
    "Kort samengevat:\n",
    "\n",
    "- Een hoge `learn rate` weegt de huidige kennis van het status-actie paar minder zwaar mee dan de nieuwe opgedane kennis, en vice versa. Op het begin moet de `learn rate` dus wel hoger dan 0 zijn om uberhaupt kennis op te doen.\n",
    "- Een hoge `discount factor` weegt een mogelijke toekomstige `reward` die behoort bij een toekomstige status-actie paar zwaar mee, en vice versa. \n",
    "    \n",
    "    Bijvoorbeeld: De speler heeft 11 punten in handen. Het algoritme kiest voor een `HIT` actie en de speler heeft vervolgens 21 punten. De beloning voor deze `HIT` actie kan dan in deze situatie sterk worden verhoogd als de `discount factor` hoog is geconfigureerd. Want de toekomstige reward binnen deze volgende state zal bij een `STAND` actie waarschijnlijk erg hoog liggen. Maar of dit een goed idee is ligt aan jou... \n",
    "\n",
    "***OPDRACHT:*** *Configureer de leerparameters en voer de onderstaande blokken codes uit om je algoritme te trainen.*\n",
    "\n",
    "Dit onderstaande blok bevat functies die nodig zijn om de kennis voor het algoritme op te kunnen bouwen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(Q_table, state, epsilon):\n",
    "    \"\"\"\n",
    "    Pick a random action (to explore options) or pick the action with the highest weight from the Q-Table (to exploit prior\n",
    "    knowledge), depending on the epsilon value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_table : dict\n",
    "        The Q-Table containing the action-state weights.\n",
    "    \n",
    "    state : str\n",
    "        A textual representation of the game state.\n",
    "    \n",
    "    epsilon : float\n",
    "        A number between 0-1. It decides whether to use the Q-Table weights to determine the next action (exploitating), or to\n",
    "        take a random action (exploration). 0 = always use the Q-Table weights, 1 = always take a random action.\n",
    "        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PlayerAction\n",
    "        The player action to perform.\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        actions = Q_table.get(state.__str__())\n",
    "        if actions is None:\n",
    "            return random.choice(ACTIONS)\n",
    "        else:\n",
    "            return PlayerAction(np.argmax(actions))\n",
    "        \n",
    "def get_q_table_actions(Q_table, state):\n",
    "    \"\"\"\n",
    "    Get the action weights for a specific state from the Q-Table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_table : dict\n",
    "        The Q-Table containing the action-state weights.\n",
    "        \n",
    "    state : str\n",
    "        A textual representation of the game state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list<float>\n",
    "        The action weights for a specific state.\n",
    "    \"\"\"\n",
    "    actions = Q_table.get(state.__str__())\n",
    "    if actions is None:\n",
    "        actions = np.zeros(len(ACTIONS))\n",
    "    return actions\n",
    "\n",
    "def update_q_table(Q_table, state, action, learn_rate, reward, discount_factor, next_state):\n",
    "    \"\"\"\n",
    "    Update the Q-Table weight of a specific action-state combination (:param action:, :param state:) by calculating the new\n",
    "    weight with the Q-function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_table : dict\n",
    "        The Q-Table containing the action-state weights.\n",
    "        \n",
    "    state : str\n",
    "        A textual representation of the game state.\n",
    "    \n",
    "    action : PlayerAction\n",
    "        A numeric representation of the player action.\n",
    "    \n",
    "    learn_rate : float\n",
    "        A number between 0-1. Also known as alpha. It determines to what proportion to weigh in the prior and new knowledge.\n",
    "        0 = discard the newly gained knowledge, 1 = discard the prior knowledge\n",
    "    \n",
    "    reward : float\n",
    "        A positive or negative reward that is associated with the chosen action (:param action:).\n",
    "    \n",
    "    discount_factor : float\n",
    "        A number between 0-1. Also known as gamma. It determines to what proportion to weigh in the future reward.\n",
    "        0 = never weigh in the future rewards, 1 = fully weigh in the future rewards.\n",
    "    \n",
    "    next_state : str\n",
    "        A textual representation of the next (future) game state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The Q-Table containing the updated action-state weights.\n",
    "    \"\"\"\n",
    "    actions = get_q_table_actions(Q_table, state)\n",
    "    next_actions = get_q_table_actions(Q_table, next_state)\n",
    "    actions[action.value] = (1 - learn_rate) * actions[action.value] + learn_rate * (reward + discount_factor * np.max(next_actions))\n",
    "    Q_table[state.__str__()] = actions\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En het volgende blok code traint het algoritme en bouwt de kennis op in een dictionary object genaamd Q (kort voor de algemene term Q-Table afkomstig van Q learning).\n",
    "\n",
    "**Zorg ervoor dat je hieronder de TODO voor de `calculate_reward()` functie nog verwerkt zodat dit voldoet aan de signature van jouw geschreven functie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = dict() # the state-action knowledge space in the form of a dictionary\n",
    "\n",
    "wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "victory_rates = [] # the victory rates per round\n",
    "ep_rewards = [] # the average reward per round\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    current_state_ = game.next_round()\n",
    "    current_state = State(current_state_)\n",
    "    total_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    rewards = []\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(Q, current_state, max(min_epsilon, epsilon * epsilon_decay ** episode))\n",
    "        next_state_, round_state = game.act(action)\n",
    "        next_state = State(next_state_)\n",
    "        reward = calculate_reward() # TODO : add the required parameters from your calculate_reward function\n",
    "        Q = update_q_table(Q, current_state, action, max(min_learn_rate, learn_rate * learn_rate_decay ** episode),\\\n",
    "                           reward, discount_factor, next_state)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        done = round_state.has_round_ended\n",
    "        current_state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            player_victory_state = round_state.player_victory_state\n",
    "            if player_victory_state == PlayerVictoryState.WON:\n",
    "                wins += 1\n",
    "            elif player_victory_state == PlayerVictoryState.DRAW:\n",
    "                draws += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "            \n",
    "            victory_rates.append([\n",
    "                wins / (episode + 1) * 100,\n",
    "                losses / (episode + 1) * 100,\n",
    "                draws / (episode + 1) * 100\n",
    "            ])\n",
    "    \n",
    "    ep_rewards.append(sum(rewards) / len(rewards))\n",
    "    print(f\"Episode {episode + 1} reward: {total_reward}\")\n",
    "\n",
    "print(f\"Wins: {wins / number_of_episodes * 100}% - Losses: {losses / number_of_episodes * 100}% - Draws: {draws / number_of_episodes * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 5: Test de kwaliteit van het algoritme\n",
    "\n",
    "Nu dat je een algoritme hebt getraind is het tijd om de kwaliteit van het algoritme te testen. Om wat meer inzage in het getrainde algoritme te krijgen, kun je de volgende functies gebruiken om het een en ander te visualiseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjack_test import print_q_table, plot_victory_rates, plot_rewards, test_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_q_table(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_victory_rates(victory_rates, number_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(ep_rewards, factor=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Opdracht:*** *Test je algoritme met de onderstaande functie. Het uiteindelijke doel is om een win/draw ratio van rond de 49-53% te krijgen. Als dit niet is gelukt, probeer dan de uitwerking van de vorige stappen aan te passen.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_algorithm(game, Q, State, show_steps=False) # show_steps=True logs every action the actor makes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
